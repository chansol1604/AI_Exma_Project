# -*- coding: utf-8 -*-
"""Example02_principles_of_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fx2VnV9B3lV64w2TA4A4kqksPN8J6MC1

#그래프 연산

* 세션이 모두 종료 된 후에 다시 실행을 해주기 위해선 [런타임 - 모두 실행]을 누르면 된다.
"""

class add_graph:
    def __init__(self):
        pass

    def forward(self,x,y):  # 실제 덧셈 함수
        out = x+y
        return out

    def backward(self,dout):    #dout =dz
        dx = 1*dout             # d(z = x+y)/dx => dz = dx
        dy = 1*dout             # d(z = x+y)/dy => dz = dy
        return dx,dy

from re import X
class mul_graph:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x*y
        return out

    def backward(self,dout):    # out = z = x*y
        dx = self.y*dout        # d(z=x*y)/dx => dx=y*dz
        dy = self.x*dout        # d(z=x*y)/dy => dz/dy=dx
        return dx,dy

class mse_graph():
    def __init__(self):
        self.loss = None
        self.x = None
        self.y = None
        self.t = None

    def forward(self,y,t):
        self.t = t      # 정답
        self.y = y      # 예측값
        self.loss= np.square(self.t - self.y).sum()/self.t.shape[0]     # self.t.shape[0]: 개수
        return self.loss

    def backward(self, x, dout =1):
        data_size = self.t.shape[0]     # 0번 인덱스에 t의 데이터 갯수만큼 들어있다(???)
        dweight_mse = ((self.y - self.t)*x).sum()*2/data_size   # weight:가중치(=커널,a) // y=ax+b, 정답 y=t, mse 편미분 검색해서 참조
        dbias_mse = (self.y - self.t).sum()*2/data_size         # b: 바이어스(bias), 편향
        return dweight_mse, dbias_mse

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_graph = mul_graph()
mul_orange_graph = mul_graph()
add_apple_orange_graph = add_graph()
mul_tax_graph = mul_graph()

apple_price = mul_apple_graph.forward(apple, apple_num)
orange_price = mul_orange_graph.forward(orange, orange_num)
all_price = add_apple_orange_graph.forward(apple_price, orange_price)
total_price = mul_tax_graph.forward(all_price,tax)
print(total_price)  # 715.0000000000001. float로 변환하면서 발생하는 오차.

# 참조: <책> 밑바닥부터 시작하는 딥러닝
# backward
dprice = 1
dall_price, dtax=mul_tax_graph.backward(dprice)
dapple_price, dorange_price = add_apple_orange_graph.backward(dall_price)
dorange, dorange_num = mul_orange_graph.backward(dorange_price)
dapple, dapple_num = mul_apple_graph.backward(dapple_price)
print('dApple',dapple)
print('dApple_num',dapple_num)
print('dOrange',dorange)
print('dOrange_num',dorange_num)

# 결과값
# dApple 2.2                        결과값에 영향을 미치는 사과값의 가중치
# dApple_num 110.00000000000001     사과의 갯수가 1증가하면 최종값에 110만큼 증가한다를 의미
# dOrange 3.3000000000000003
# dOrange_num 165.0

"""**섭씨, 화씨 구하는... **"""

# 모델 만들기x, 데이터 만들기 위해서 추가
import numpy as np

def celcius_to_fahrenhe(x):
    return x*1.8+32

# 모델 만들기
weight = np.random.uniform(0,5,1)       # normal:평균과 표준편차를 이용해서 정규분포를 가지고 하는 것. uniform: 0~5까지 균일하게!
print(weight)
bias = 0
data_C = np.arange(1,100)
data_F = celcius_to_fahrenhe(data_C)
scaled_data_C = data_C/100
scaled_data_F = data_F/100
print(scaled_data_C)
print(scaled_data_F)

weight_graph = mul_graph()
bias_graph = add_graph()

weighted_data = weight_graph.forward(weight,scaled_data_C)      # weight랑 섭씨온도 곱한 값 100개
predict_data = bias_graph.forward(weighted_data,bias)
print(predict_data)

dout = 1
dbias, dbiased_data = bias_graph.backward(dout)
dweight, dscaled_data_C = weight_graph.backward(dbiased_data)
print(dbias)
print(dweight)

mseGraph = mse_graph()
mse = mseGraph.forward(predict_data, scaled_data_F)
print(mse)

weight_mse_gradient, bias_mse_gradient = mseGraph.backward(scaled_data_C)
print(weight_mse_gradient)
print(bias_mse_gradient)

learning_rate = 0.1     # 적당한 값 조절이 중요, 너무 작게 주면 학습률이 떨어져서 오래 걸리고 너무 크게 주면 발산함
learned_weight = weight - learning_rate*weight_mse_gradient*np.average(dweight)        # tip) 줄바꿈하고 싶으면 하고자 하는 위치에 \를 입력하고 줄바꿈을 하면 된다.
print('before learning weight: ',weight)
print('after learning weight: ',learned_weight)

learned_bias = bias - learning_rate * bias_mse_gradient * dbias
print('before learning bias :', bias)
print('after learning bias :', learned_bias)

error_list = []
weight_list = []
bias_list = []
for i in range(1000):
  #forward
  weighted_data = weight_graph.forward(weight,scaled_data_C)    # weight * scaled_data_C
                                                              # scaled_data_C = np.arange(1,100)/100
  predicted_data = bias_graph.forward(weighted_data, bias)      # weighted_data + bias
                                                              # bias = 0
  #backward
  dout = 1
  dbias, dbiased_data = bias_graph.backward(dout)         # 1 * dout, 1 * dout
  dweight, dscaled_data_C = weight_graph.backward(dbiased_data) # self.y * dbiased_data, self.x * dbiased_data
  #mse
  mse = mseGraph.forward(predicted_data, scaled_data_F)   # self.loss= np.square(scaled_data_F - predicted_data).sum() / scaled_data_F의 개수
  error_list.append(mse)
  weight_mse_gradient, bias_mse_gradient = mseGraph.backward(scaled_data_C)
  #learning
  weight_list.append(weight)
  weight = weight-learning_rate *weight_mse_gradient * np.average(dweight)
  bias_list.append(bias)
  bias = bias - learning_rate * bias_mse_gradient * dbias

weight_list.append(weight)
bias_list.append(bias)
print(weight)   # [1.80181889]
print(bias)     # 0.31905645139429517

print(error_list[-1])

import matplotlib.pyplot as plt

plt.plot(error_list)
plt.show()

plt.plot(weight_list)

plt.plot(bias_list)   # 초반에 더 멀어지다가 다시 제자리를 찾는 것을 확인할 수 있다.
plt.show()            # wieght 값과 따로 계산해야 하는 이유