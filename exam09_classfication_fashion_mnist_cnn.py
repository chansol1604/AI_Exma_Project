# -*- coding: utf-8 -*-
"""exam09_classfication_fashion_mnist_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KV36h50jiLPzjcezz-Jt0b8FdaJQZWZO
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Activation
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout
from tensorflow.keras.optimizers import Adam
from keras.utils import np_utils
from tensorflow.keras import datasets

(X_train, Y_train), (X_test, Y_test) = datasets.fashion_mnist.load_data()
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

label = ['T_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']

my_sample = np.random.randint(60000)
plt.imshow(X_train[my_sample], cmap = 'gray')       # img 출력, cmap = 'gray' => 흑백
plt.show()
print(label[Y_train[my_sample]])
print(X_train[my_sample])

y_train = np_utils.to_categorical(Y_train)
y_test = np_utils.to_categorical(Y_test)
print(Y_train[5000])        # 출력값 : 7
print(y_train[5000])        # 출력값 : [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]

x_train = X_train / 255                   # minmax 스케일링, 최대값으로 나눈 것
x_test = X_test / 255
x_train = x_train.reshape(60000, 28, 28, 1)   # -1을 사용해도 됨
x_test = x_test.reshape(-1, 28, 28, 1)
print(x_train.shape)

"""참고

https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53
"""

model = Sequential()
model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))
# 필터가 32장, kenel_size -> 필터의 사이즈
# 즉, 3*3 필터가 32장
# padding = 'same' -> 입력 이미지와 출력 이미지 사이즈를 똑같이, 원래 이미지에 0값의 테두리를 둘러서 필터를 할 수 있게 해줌
# 최종적으로 원래 사이즈 크기의 32개의 이미지가 생성, 느려지는 이유.
# (커널 사이즈)3 * 3 * 32 + (출력) 1 *32 = 320
model.add(MaxPool2D(padding = 'same', pool_size = (2,2)))
# (2*2) 크기에서 가장 큰 값만 뽑아서 그 값만 남김, 이번에는 중복 계산 하지 않음
# padding 사용 시 홀수의 사이즈 경우 한 셀을 추가함
# maxpool 연산 끝나고 이미지 크기가 반으로 줄
model.add(Conv2D(32, kernel_size = (3,3), padding = 'same', activation = 'relu'))
# 32장이 출력됨, 필터 32개와 쌍을 이룸.
# 최종적으로 나온 32장의 데이터를 모두 합쳐서 1장만 반환
# (9 * 32 ) * 32 + 1 s* 32
model.add(MaxPool2D(padding = 'same', pool_size = (2,2)))
# 사용하지 않아도 conv2D 뒤에 따라 붙는다. 사용하지 않는 경우 pool_size(1,1)을 주면 됨
model.add(Flatten())
# reshape 해주는 것, 32장의 픽셀을 한 장으로 만듬.(7 * 7 (사이즈) * 32) = 1568
model.add(Dropout(0.2))
model.add(Dense(128, activation = 'relu'))
# 1568 * 128 + 128 = 200832
model.add(Dropout(0.2))
model.add(Dense(10, activation = 'softmax'))
model.summary()

opt = Adam(lr = 0.01)
model.compile(opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])

fit_hist = model.fit(x_train, y_train, batch_size = 128, epochs = 15, validation_split = 0.2, verbose =1)

score = model.evaluate(x_test, y_test, verbose = 0)
print("Final tet set accuracy :", score[1])

plt.plot(fit_hist.history['accuracy'])
plt.plot(fit_hist.history['val_accuracy'])
plt.show()

my_sample = np.random.randint(10000)
plt.imshow(X_test[my_sample], cmap = 'gray')
print(label[Y_test[my_sample]])
pred = model.predict(x_test[my_sample].reshape(-1, 28, 28, 1))
print(pred)
print(label[np.argmax(pred)])

